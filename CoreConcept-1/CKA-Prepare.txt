The purpose of kubernetes is to host your applications in the form of containers in an automated function 
so that you can easily deploy as many instances of your application as required and easily enable communication
between different services with your application.

cluster architecture
####################
kubernetes architecture
etcd for beginners
etcd in kubernetes
kube-api server
controller managers
kube scheduler
kublet
kube proxy

master node - manage, plan, schedule, monitor nodes
worker node - host application as containers

etcd -  There are many containers being loaded and unloaded from the ships on a daily basics. And so you need to maintain information about the different ships
and what container is on which ship , what time it was loaded etc. All of these are stored in hight available key value know as 'etcd'.
etcd is a database that stores information in a key-value format.

shedulers - when ships arrive you load containers on them using cranes the cranes identify the containers that need to be placed on ships. it identifies the right ship
based on its size its capacity the number of containers already on the ship and any other contditions such as the destination of the ship. the type of containers
it is allowed to carry etc. so those are schedulers in a kubernetes cluster. the schedulers identifies the right node to place a container on based on the containers. 

kubeapi server - kubeapi server is the primary management component of kubernetes. the kube-api server is responsible for orchestrating all operations within the cluster. It
exposes the kubernetes API which is used by externals users to perform management operations on the cluster as well as the various controllers to monitor the state 
of the cluster and make necessary changes as required and by the worker nodes to communicate with the server.  

master
etcd - etcd cluster which stores information about the cluster
sheduler - we have the kube scheduler that is responsible for scheduling applicationis or containers on Nodes.
controllers - we have different controllers that take care of different functions like the node control, replication
kubeapi server - we have the kubeapi server that is responsible for orchestrating all operations within the cluster.

worker 
kubelet - we have the kubelet that listens for instructions from the kube-apiserver and manages containers and the kube proxy. That helps in enabling communication between services with the cluster.  

==================================================================================================================================

etcd for beginner
-it is distributed reliable key value store that is simple secure and fast
-when you run etcd it starts a service that listens on port 2379 by default. YOou can then attach any client to the etcd service to store and retrieve information. a default that comes with etcd is the etcd control client
-the etcdctl is a command line client for etcd.
-you can use it to store and retrieve key-value pairs to store a key-value pair run the etcdct set key1 command foollowed by the value value one.
-> etcdctl set key1 value1
-> etcdctl get key1		//the output is value1

etcd in kubernetes
-the etcd datastore stores information regarding the cluster such as the nodes, pods, configs, secrets, accounts, roles, bindings and others. 
-every information you see when you run the kubectl get command is from the etcd server. 
-every change you make to your cluster such as edding additional nodes, deploying pods or replica sets are updated in the etcd server. 

two types of kubernetes deployment.
scratch - download and run
kubeadm tool - testing enviro
-the advertised client url (This is the address on which listens. It happens to be on the IP of the server and on port 2379 which is the default port on ehich etcd listens.)
-this is the url that should be configured on the kube-api server when it tries to reach the etcd server.
-> kubectl get pods -n kube-system
-> kubectl exec etcd-master -n kube-system etcdctl get / --prefix -keys-only		//to list all keys stored by kubernetes

=============================================================================================

kubeapi server
steps
1. authenticate user
2. validate request
3. retrieve data
4. update etcd
5. scheduler
6. kubelet

-when you run a kubectl command, the kubectl utility is infact reaching to the kube-apiserver.
-the kube-api server first authenticates the request and validates it. It then retrieves the data from the etcd cluster and responds back with the request information.

-the sheduler continuously monitors the API server and realizes that there is a new pod with no node assigned.
-the scheduler identifies the right node to place the new POD on and communicates that bacck to the kube-api server.
-the api server the updates the information in the etcd cluster. The api server then passes that information to the kubelet in appropriate worker node.
-the kubelet then creates the POD on the node and constructs the container runtime engine to deploy the application images.
-one done, the kubelet updates the status back to the API server and API server the updates the data back in the etcd cluster. 

-the kube-api server is the only component that interacts directly with the etcd data store.
->kubectl get pods -n kube-system		//view api-server - kubeadm
->cat /etc/kubernetes/manifests/kube-apiserver.yaml		//view api-server options
->cat /etc/systemd/system/kube-apiserver.service 

======================================================================================================

controller
controller is a process that continuously monitors the state of various components within the system and works toward bringing the whole system to the desired functioning state.
node-controller
replication-controller 
-> kubectl get pods -n kube-system
-> cat /etc/kubernetes/manifests/kube-controller-manager.yaml
-> cat /etc/systemd/system/kube-controller-manager.service 

===============================================================================

scheduler
-the scheduler is only responsibel for deciding which pod goes on which node. it doesnt actually place the pod on the nodes.
-the scheduler only decides which pod goes where.
-> cat /etc/kubernetes/manifests/kube-scheduler.yaml

===========================================================================================

kubelet
-register node
-create PODs
-Monitor Node & PODSs

======================================================

kubeproxy
-kubeproxy is a process that runs on each node in the kubernetes cluster.
-it job is to look for new services and every time a new service is created it creates the appropriate rules on each node to forward traffic to those services to the backend pods. 
-one way it does this is using iptables rules.
-> kubectl get pods -n kube-system
-> kubectl get daemonset -n kube-system

======================================================
replicaset
replicas: 6
kubectl replace -f replicaset-definition.yml
kubectl scale --replicas=6 replicaset-definition.yml
kubectl scale --replicas=6 replicaset myapp-replicaset

kubectl edit replicaset new-replica-set

Deployment
Once create the deployment, replicaset is already in 

Create a new Deployment with the below attributes using your own deployment definition file
Name: httpd-frontend; Replicas: 3; Image: httpd:2.4-alpine

=================================================

Namespaces in kubernetes
there are two boys 
1. mark smith
2. mark williams

default namespaces include pods, service, deployment
kubesystem namespaces prevent from the user accidentally deteting or modifying these services 
kubepublic namespaces where resources that should be made available to all users are created.
you can create your own namespaces

DNS
mysql.connect('db-service.dev.svc.cluster.local')
db-service = service name
dev = namespace
svc = service
cluster.local = domain

->kubectl get pods 					//only show default namespace
->kubectl get pods --namespace=kube-system		//view by namespace

->kubectl create -f pod-definition.yml			//create pod in default namepace
->kubectl create -f pod-definition.yml --namespace=dev	//create pod in dev namespace

Create namespace (first way)
apiversion: v1
kind: Namespace
metadata:
  name: dev
->kubectl create -f .

create namespace(2nd way)
->kubectl create namespace dev

if we have three namespace dev,default,prod
->kubectl get namespace
->kubectl get pods --namespace=dev		//only show dev namespace
->kubectl get pods 
->kubectl get pods --namespace prod		//only show prod namespace
->kubectl config set-context $(kubectl config current-context) --namespace=dev		//change dev namespace to default cmd list
->kubectl get pods				//we can see dev namespace
->kubectl get pods --all-namespaces

->kubectl run redis --image=redis --generator=run-pod/v1 -n finance			//create pod in random custom namspace


=========================================================

services 
NodePort
ClusterIP
LoadBalancer

kubectl create -f service-definition.yml
kubectl get services

=========================================================

for exam



While you would be working mostly the declarative way - using definition files, imperative commands can help in getting one time tasks done quickly, as well as generate a definition template easily. This would help save considerable amount of time during your exams.

Before we begin, familiarize with the two options that can come in handy while working with the below commands:

--dry-run: By default as soon as the command is run, the resource will be created. If you simply want to test your command , use the --dry-run option. This will not create the resource, instead, tell you weather the resource can be created and if your command is right.

-o yaml: This will output the resource definition in YAML format on screen.



Use the above two in combination to generate a resource definition file quickly, that you can then modify and create resources as required, instead of creating the files from scratch.



POD
Create an NGINX Pod

kubectl run --generator=run-pod/v1 nginx --image=nginx



Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

kubectl run --generator=run-pod/v1 nginx --image=nginx --dry-run -o yaml



Deployment
Create a deployment

kubectl create deployment --image=nginx nginx



Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

kubectl create deployment --image=nginx nginx --dry-run -o yaml



Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) with 4 Replicas (--replicas=4)

kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml

The usage --generator=deployment/v1beta1 is deprecated as of Kubernetes 1.16. The recommended way is to use the kubectl create option instead.



IMPORTANT:

kubectl create deployment does not have a --replicas option. You could first create it and then scale it using the kubectl scale command.



Save it to a file - (If you need to modify or add some other details)

kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml > nginx-deployment.yaml



OR

kubectl create deployment --image=nginx nginx --dry-run -o yaml > nginx-deployment.yaml

You can then update the YAML file with the replicas or any other field before creating the deployment.



Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379

kubectl expose pod redis --port=6379 --name redis-service --dry-run -o yaml

(This will automatically use the pod's labels as selectors)

Or

kubectl create service clusterip redis --tcp=6379:6379 --dry-run -o yaml (This will not use the pods labels as selectors, instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service)



Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:

kubectl expose pod nginx --port=80 --name nginx-service --dry-run -o yaml

(This will automatically use the pod's labels as selectors, but you cannot specify the node port. You have to generate a definition file and then add the node port in manually before creating the service with the pod.)

Or

kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run -o yaml

(This will not use the pods labels as selectors)

Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot accept a node port. I would recommend going with the `kubectl expose` command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.



Reference:

https://kubernetes.io/docs/reference/kubectl/conventions/


===================================================================================================


Deploy a pod named nginx-pod using the nginx:alpine image.
Use imperative commands only
ans: kubectl run --generator=run-pod/v1 nginx-pod --image=nginx:alpine

Deploy a redis pod using the redis:alpine image with the labels set to tier=db.


Either use imperative commands to create the pod with the labels. 
Or else use imperative commands to generate the pod definition file, then add the labels before creating the pod using the file.
Ans: kubectl run --generator=run-pod/v1 redis --image=redis:alpine -l tier=db


Create a service redis-service to expose the redis application within the cluster on port 6379.
Use imperative commands
Ans: kubectl expose pod redis --port=6379 --name redis-service


Create a deployment named webapp using the image kodekloud/webapp-color with 3 replicas
Try to use imperative commands only. Do not create definition files.
Ans: Use the command kubectl create deployment webapp --image=kodekloud/webapp-color.
     The scale the webapp to 3 using command kubectl scale deployment/webapp --replicas=3

